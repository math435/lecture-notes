{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Some useful $\\LaTeX$ commands are defined in this cell:\n",
    "$$\n",
    "\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\paren}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\brack}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\ip}[2]{\\left\\langle#1,#2\\right\\rangle}\n",
    "\\DeclareMathOperator{\\span}{span}\n",
    "\\DeclareMathOperator{\\fl}{fl}\n",
    "\\abs{x}, \\norm{x}, \\set{x}, \\paren{x}, \\brack{x}, \\ip{x}{y}, \\span, \\fl\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 14.4 Roundoff and data errors in numerical differentiation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above we have seen that the actual error degrades when $h$ becomes too small due to roundoff error.\n",
    "\n",
    "This is happening since, in each of the formulas we have looked at, we are subtracting values that are very nearly equal, leading to **catastrophic cancellation** and a significant loss of precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall:\n",
    "\n",
    "The error for the **central divided difference** satisfies:\n",
    "\n",
    "$$\\abs{f'(x_0) -  \\frac{f(x_0 + h) - f(x_0 - h)}{2h}} = \\frac{h^2}{6}\\abs{f'''(\\xi)}, \\quad \\text{for some $\\xi \\in (x_0-h,x_0+h).$}$$\n",
    "\n",
    "Thus, when $f'''(x_0) \\neq 0$, we have that\n",
    "\n",
    "$$\\abs{f'(x_0) -  \\frac{f(x_0 + h) - f(x_0 - h)}{2h}} \\approx \\frac{h^2}{6}\\abs{f'''(x_0)}, \\quad \\text{for $h > 0$ small.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "We will use the function \n",
    "\n",
    "$$f(x) = e^x \\sin(3x)$$\n",
    "\n",
    "for the numerical examples in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)  = exp(x)*(                sin(3x))\n",
    "f1(x) = exp(x)*(  3cos(3x) +    sin(3x))\n",
    "f2(x) = exp(x)*(  6cos(3x) -   8sin(3x))\n",
    "f3(x) = exp(x)*(-18cos(3x) -  26sin(3x))\n",
    "f4(x) = exp(x)*(-96cos(3x) +  28sin(3x))\n",
    "f5(x) = exp(x)*(-12cos(3x) + 316sin(3x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LaTeXStrings\n",
    "\n",
    "x0 = 0.4\n",
    "h = [10.0^-i for i=0:0.1:20]\n",
    "approxfp = (f.(x0.+h) - f.(x0.-h))./(2h)\n",
    "err = abs.(f1(x0) .- approxfp)\n",
    "p_err = h.^2*abs(f3(x0))/6\n",
    "\n",
    "plot(legend=:bottomleft, xaxis=:log, yaxis=:log,\n",
    "    xlabel=L\"h\", ylabel=\"Absolute error\", ylims=(1e-15, 1e1))\n",
    "plot!(h, err, label=\"Actual error\")\n",
    "plot!(h, p_err, ls=:dash, label=\"Predicted error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see the effect of roundoff error on the approximation error.\n",
    "\n",
    "It would be nice to know what is the **optimal value** of $h$. In this example, it appears that the best $h$ is around $10^{-6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the roundoff error\n",
    "\n",
    "Let $\\bar{f}(x) \\equiv \\fl(f(x))$, and define\n",
    "\n",
    "$$\n",
    "D_h = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}, \\qquad \n",
    "\\bar{D}_h = \\frac{\\bar{f}(x_0 + h) - \\bar{f}(x_0 - h)}{2h}.\n",
    "$$\n",
    "\n",
    "Define the **roundoff error in the calculation of $f$** as\n",
    "\n",
    "$$e_r(x) = \\bar{f}(x) - f(x)$$\n",
    "\n",
    "and suppose that $\\abs{e_r(x)} \\leq \\varepsilon$, where $\\varepsilon$ is some small multiple of the **unit-roundoff** $\\eta$ (recall that $\\eta \\approx 1.1 \\times 10^{-16}$ for `Float64`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = eps(Float64)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, suppose that $\\abs{f'''(\\xi)} \\leq M$, for all $\\xi \\in (x_0 - h, x_0 + h)$. Thus,\n",
    "\n",
    "$$\\abs{f'(x_0) - D_h} = \\frac{h^2}{6}\\abs{f'''(\\xi)} \\leq  \\frac{h^2M}{6}.$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\abs{f'(x_0) - \\bar{D}_h}\n",
    "&= \\abs{f'(x_0) - D_h + D_h - \\bar{D}_h} \\\\\n",
    "&\\leq \\abs{f'(x_0) - D_h} + \\abs{D_h - \\bar{D}_h} \\\\\n",
    "&\\leq  \\frac{h^2M}{6} + \\abs{D_h - \\bar{D}_h}. \\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's bound $\\abs{D_h - \\bar{D}_h}$. We have\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\abs{D_h - \\bar{D}_h}\n",
    "&= \\abs{ \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{\\bar{f}(x_0 + h) - \\bar{f}(x_0 - h)}{2h} } \\\\\n",
    "&= \\frac{1}{2h}\\abs{ f(x_0 + h) - f(x_0 - h) - \\bar{f}(x_0 + h) + \\bar{f}(x_0 - h) } \\\\\n",
    "&= \\frac{1}{2h}\\abs{ e_r(x_0 - h) - e_r(x_0 + h) } \\\\\n",
    "&\\leq \\frac{1}{2h}\\paren{\\abs{ e_r(x_0 - h)} + \\abs{ e_r(x_0 + h) }} \\\\\n",
    "&\\leq \\frac{1}{2h} 2\\varepsilon \\\\\n",
    "&\\leq \\frac{\\varepsilon}{h}. \\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "\n",
    "$$\\abs{f'(x_0) - \\bar{D}_h} \\leq \\frac{h^2M}{6} + \\frac{\\varepsilon}{h}.$$\n",
    "\n",
    "Initially, when $h$ is rather large, the roundoff error term $\\frac{\\varepsilon}{h}$ will be much smaller than the approximation error (a.k.a. **truncation error**) term $\\frac{h^2M}{6}$.\n",
    "\n",
    "However, as $h$ becomes small, $\\frac{\\varepsilon}{h}$ eventually becomes larger than $\\frac{h^2M}{6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 0.4\n",
    "\n",
    "ɛ = 3η  # Played around with the multiple to \n",
    "        # obtain a valid bound\n",
    "\n",
    "M = abs(f3(x0))\n",
    "\n",
    "h = [10.0^-i for i=0:0.1:20]\n",
    "approxfp = (f.(x0.+h) - f.(x0.-h))./(2h)\n",
    "err = abs.(f1(x0) .- approxfp)\n",
    "p_err = h.^2*M/6\n",
    "r_err = ɛ./h\n",
    "\n",
    "plot(legend=:bottomleft, xaxis=:log, yaxis=:log,\n",
    "    xlabel=L\"h\", ylabel=\"Absolute error\", ylims=(1e-15, 1e1))\n",
    "plot!(h, err, label=\"Actual error\")\n",
    "plot!(h, p_err, ls=:dash, label=\"Truncation error\")\n",
    "plot!(h, r_err, ls=:dash, label=\"Roundoff error\")\n",
    "plot!(h, p_err + r_err, label=\"Error bound\", c=:red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"optimal\" $h$ for the central divided difference formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define\n",
    "\n",
    "$$E(h) = \\frac{h^2M}{6} + \\frac{\\varepsilon}{h}.$$\n",
    "\n",
    "We want to find the minimum value of $E(h)$. First we solve $E'(h) = 0$ to find the critical points:\n",
    "\n",
    "$$E'(h) = \\frac{hM}{3} - \\frac{\\varepsilon}{h^2} = 0 \\quad \\implies \\quad h^3 = \\frac{3\\varepsilon}{M} \\quad \\implies \\quad h = \\sqrt[3]{\\frac{3\\varepsilon}{M}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = cbrt(3ε/M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check if this critical point is a local minimizer or maximizer:\n",
    "\n",
    "$$E''(h) = \\frac{M}{3} + \\frac{2\\varepsilon}{h^3} \\quad \\implies \\quad E''\\paren{\\sqrt[3]{\\frac{3\\varepsilon}{M}}} = \\frac{M}{3} + \\frac{2\\varepsilon}{\\frac{3\\varepsilon}{M}} = M > 0.$$\n",
    "\n",
    "Thus, the critical point $h = \\sqrt[3]{\\frac{3\\varepsilon}{M}}$ is a local minimizer, and the local minimum is\n",
    "\n",
    "$$E\\paren{\\sqrt[3]{\\frac{3\\varepsilon}{M}}} = \\frac12  \\sqrt[3]{M} (3\\varepsilon)^{\\frac{2}{3}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E(h) = h^2*M/6 + ε/h\n",
    "E(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Printf\n",
    "\n",
    "x0 = 0.4\n",
    "\n",
    "ɛ = 3η  # Played around with the multiple to obtain a valid bound\n",
    "M = abs(f3(x0))\n",
    "\n",
    "h = [10.0^-i for i=0:0.1:20]\n",
    "approxfp = (f.(x0.+h) - f.(x0.-h))./(2h)\n",
    "err = abs.(f1(x0) .- approxfp)\n",
    "p_err = h.^2*M/6\n",
    "r_err = ɛ./h\n",
    "\n",
    "hopt = cbrt(3ɛ/M)\n",
    "Emin = .5cbrt(M)*(3ɛ)^(2/3)\n",
    "\n",
    "plt = plot(legend=:bottomleft, xaxis=:log, yaxis=:log,\n",
    "    xlabel=L\"h\", ylabel=\"Absolute error\", ylims=(1e-15, 1e1))\n",
    "plot!(h, err, label=\"Actual error\")\n",
    "plot!(h, p_err, ls=:dash, label=\"Truncation error\")\n",
    "plot!(h, r_err, ls=:dash, label=\"Roundoff error\")\n",
    "plot!(h, p_err + r_err, label=\"Error bound\", c=:red)\n",
    "scatter!([hopt], [Emin], label=\"Optimal h\", c=:red)\n",
    "\n",
    "@printf \"optimal h = %.2e\\n\" hopt\n",
    "@printf \"Emin = %.2e\\n\" Emin\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advice on choosing $h$\n",
    "\n",
    "Since the values of $\\varepsilon$ and $M$ are typically unknown, it is usually not possible to know the optimal value of $h$.\n",
    "\n",
    "Thus, we should choose a value of $h$ that is well above the point where roundoff error begins to corrupt our calculation.\n",
    "\n",
    "That is, we should choose $h$ so that the truncation error is much larger than the roundoff error.\n",
    "\n",
    "In the above example, using $h$ between $10^{-5}$ and $10^{-4}$ would be best.\n",
    "\n",
    "In general, if the order of accuracy is $q$, then choose\n",
    "\n",
    "$$h > \\eta^{1/(q+1)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η^(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = .4\n",
    "\n",
    "@printf \"%5s %14s %12s\\n\" \"h\" \"approx\" \"error\"\n",
    "for i = 0:8\n",
    "    h = 10.0^-i\n",
    "    approx = (f(x0+h) - f(x0-h))/(2h)\n",
    "    err = abs(approx - f1(x0))\n",
    "    @printf \"%5.0e %14.10f %12.5e\\n\" h approx err\n",
    "end\n",
    "\n",
    "abs(f3(x0))/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical differentiation of noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical differentiation is very sensitive to small changes in the input -- it is an **ill-conditioned** problem.\n",
    "\n",
    "In the following numerical test, we will see that just a small amount of noise in the function values can create large errors in the approximation of its derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = .01  # noise factor = 1%\n",
    "n = 200\n",
    "\n",
    "xx = range(0, 1, length=n)\n",
    "h = xx[2] - xx[1]\n",
    "\n",
    "yy = f.(xx) # Noiseless data\n",
    "yyp = (yy[3:n] .- yy[1:n-2])./(2h)  # central divided difference\n",
    "\n",
    "yyn = (1 .+ nf*randn(size(yy))).*yy  # add noise to function values\n",
    "yynp = (yyn[3:n] .- yyn[1:n-2])./(2h) # central divided difference\n",
    "\n",
    "yyptrue = f1.(xx[2:n-1]) # True derivative\n",
    "\n",
    "# Create 2x1 array of subplots\n",
    "plt = plot(layout=(1,2), size=(700,350), legend=:bottomleft, ylim=(-10,6))\n",
    "\n",
    "plot!(subplot=1, title=\"Noiseless differentiation\")\n",
    "plot!(xx, yy, label=\"True f(x)\")\n",
    "plot!(xx[2:n-1], yyp, label=\"Numerical f'(x)\")\n",
    "plot!(xx[2:n-1], yyptrue, label=\"True f'(x)\")\n",
    "\n",
    "plot!(subplot=2, title=\"Noisy differentiation\")\n",
    "plot!(xx, yyn, label=\"Noisy f(x)\", subplot=2)\n",
    "plot!(xx[2:n-1], yynp, label=\"Numerical f'(x)\", subplot=2)\n",
    "plot!(xx[2:n-1], yyptrue, label=\"True f'(x)\", subplot=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "\n",
    "xx = range(0, 1, length=n)\n",
    "h = xx[2] - xx[1]\n",
    "\n",
    "yy = f.(xx)\n",
    "yyptrue = f1.(xx[2:n-1])\n",
    "\n",
    "@printf \"%10s %16s\\n\" \"noise (%)\" \"max rel err (%)\"\n",
    "for i = -8:-1\n",
    "    nf = 10.0^i\n",
    "    \n",
    "    yyn = (1 .+ nf*randn(size(yy))).*yy  # add noise to function values\n",
    "    yynp = (yyn[3:n] .- yyn[1:n-2])./(2h) # central divided difference\n",
    "    \n",
    "    maxrelerr = maximum(abs.(yyptrue .- yynp)./abs.(yyptrue))\n",
    "\n",
    "    @printf \"%10.6f %16.2f\\n\" nf*100 maxrelerr*100\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are techniques for numerically differentiating noisy data, such as the **total-variation regularization** technique used in:\n",
    "\n",
    "> [Numerical Differentiation of Noisy, Nonsmooth Data](http://dx.doi.org/10.5402/2011/164564), Rick Chartrand, ISRN Applied Mathematics (2011).\n",
    "\n",
    "The idea is to minimize the total-variation of the derivative while ensuring that the integral of the derivative fits the original function well (in the least-squares sense). Thus, the numerical derivative is smoothed-out and does not vary as much, so is less affected by the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
