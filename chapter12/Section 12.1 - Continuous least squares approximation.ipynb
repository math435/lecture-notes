{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Some useful $\\LaTeX$ commands are defined in this cell:\n",
    "$$\n",
    "\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\paren}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\brack}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\ip}[2]{\\left\\langle#1,#2\\right\\rangle}\n",
    "\\DeclareMathOperator{\\span}{span}\n",
    "\\abs{x}, \\norm{x}, \\set{x}, \\paren{x}, \\brack{x}, \\ip{x}{y}, \\span\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 12.1 Continuous least squares approximation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "In the previous two chapters we approximated a function $f(x)$ over an interval $[a,b]$ by interpolating points $\\set{(x_i,f(x_i))}_{i=0}^n$ (and possibly values of $f'$) by either a polynomial $p_n(x)$ having degree at most $n$, or by a piecewise polynomial (e.g., a cubic spline) $p(x)$.\n",
    "\n",
    "The **error of the approximation** was measured using the **infinity-norm**:\n",
    "\n",
    "$$\n",
    "\\norm{f - p}_\\infty := \\max_{x \\in [a,b]} \\abs{f(x) - p(x)}.$$\n",
    "\n",
    "We found valid upper bounds on this error, such as \n",
    "\n",
    "$$\\norm{f - p_n}_\\infty \\leq \\frac{\\norm{f^{(n+1)}}_\\infty}{(n+1)!} \\max_{x \\in [a,b]} \\prod_{i=0}^n \\abs{x-x_i},$$\n",
    "\n",
    "for the unique polynomial $p_n(x)$ having degree at most $n$ interpolating $(x_0,f(x_0)), \\ldots, (x_n,f(x_n))$.\n",
    "\n",
    "We even looked an minimizing this upper bound, which led us to studying **Chebyshev points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best approximation\n",
    "\n",
    "We now will look at minimizing the **approximation error** directly. \n",
    "\n",
    "Given a set of **linearly independent** functions $\\set{\\phi_j(x)}_{j=0}^n$, we want to find a function \n",
    "\n",
    "$$p(x) = \\sum_{j=0}^n c_j \\phi_j(x)$$\n",
    "\n",
    "that minimizes the error $\\norm{f - p}$, where $\\norm{\\cdot}$ is a **function norm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function norms\n",
    "\n",
    "Let $\\norm{\\cdot}$ be a **norm** for $f \\in C[a,b]$.\n",
    "\n",
    "Then $\\norm{\\cdot}$ is a function that takes an input $f$ and returns a real number.\n",
    "\n",
    "A norm $\\norm{\\cdot}$ for functions must satisfy:\n",
    "\n",
    "1. $\\norm{f} \\geq 0$, for all functions $f$, and $\\norm{f} = 0$ if and only if $f(x) = 0$, for all $x \\in [a,b]$;\n",
    "\n",
    "2. $\\norm{\\alpha f} = \\abs{\\alpha} \\norm{f}$, for all $\\alpha \\in \\mathbb{R}$;\n",
    "\n",
    "3. $\\norm{f+g} \\leq \\norm{f} + \\norm{g}$ for all $f, g \\in C[a,b]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of function norms\n",
    "\n",
    "The $L_1$ norm of a function $f \\in C[a,b]$ is defined as\n",
    "\n",
    "$$\\norm{f}_1 := \\int_a^b \\abs{f(x)} dx.$$\n",
    "\n",
    "The $L_\\infty$ norm of a function $f \\in C[a,b]$ is defined as\n",
    "\n",
    "$$\\norm{f}_\\infty := \\max_{x \\in [a,b]} \\abs{f(x)}.$$\n",
    "\n",
    "We will define the $L_2$ norm in terms of the inner-product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **inner-product** of two functions $f,g \\in C[a,b]$ is defined as\n",
    "\n",
    "$$\\ip{f}{g} := \\int_a^b f(x) g(x)\\,dx.$$\n",
    "\n",
    "Note that we can use the inner-product to talk about the **angle** between functions. For example, functions $f$ and $g$ are said to be **orthogonal** if $\\ip{f}{g} = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $L_2$ norm of a function $f \\in C[a,b]$ is defined as\n",
    "\n",
    "$$\\norm{f}_2 := \\sqrt{\\ip{f}{f}} = \\paren{\\int_a^b \\brack{f(x)}^2 dx}^{\\frac12}.$$\n",
    "\n",
    "In this chapter, we will consider methods for minimizing $\\norm{f-p}_2$, also known as the **continuous least-squares problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the continuous least-squares problem\n",
    "\n",
    "We want to find the coefficients $c_0,\\ldots,c_n$ such that \n",
    "\n",
    "$$p(x) = \\sum_{j=0}^n c_j \\phi_j(x)$$\n",
    "\n",
    "minimizes $\\norm{f-p}_2$ over all functions $p \\in \\span\\set{\\phi_0,\\ldots,\\phi_n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can minimize \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\norm{f-p}_2^2 \n",
    "& = \\ip{f-p}{f-p} \\\\\n",
    "& = \\ip{f}{f} - 2\\ip{f}{p} + \\ip{p}{p} \\\\\n",
    "& = \\ip{f}{f} - 2\\ip{f}{\\sum_{j=0}^n c_j \\phi_j} + \\ip{\\sum_{j=0}^n c_j \\phi_j}{\\sum_{j=0}^n c_j \\phi_j} \\\\\n",
    "& = \\ip{f}{f} - 2\\sum_{j=0}^n c_j \\ip{f}{\\phi_j} + \\sum_{j=0}^n \\sum_{k=0}^n c_j c_k \\ip{\\phi_j}{\\phi_k} \\\\\n",
    "& = \\ip{f}{f} - 2b^Tc + c^T B c, \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $B \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ and $b, c \\in \\mathbb{R}^{n+1}$ are defined as\n",
    "\n",
    "$$\n",
    "B := \n",
    "\\begin{bmatrix}\n",
    "\\ip{\\phi_0}{\\phi_0} & \\cdots & \\ip{\\phi_0}{\\phi_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "\\ip{\\phi_n}{\\phi_0} & \\cdots & \\ip{\\phi_n}{\\phi_n} \\\\\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "b := \n",
    "\\begin{bmatrix}\n",
    "\\ip{f}{\\phi_0}\\\\\n",
    "\\vdots\\\\\n",
    "\\ip{f}{\\phi_n}\\\\\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "c := \n",
    "\\begin{bmatrix}\n",
    "c_0 \\\\\n",
    "\\vdots\\\\\n",
    "c_n \\\\\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The normal equations\n",
    "\n",
    "Let $g \\colon \\mathbb{R}^{n+1} \\to \\mathbb{R}$ be defined as\n",
    "\n",
    "$$g(c) = c^T B c - 2b^Tc + \\ip{f}{f}.$$\n",
    "\n",
    "So we want to find $c^* \\in \\mathbb{R}^{n+1}$ such that\n",
    "\n",
    "$$g(c^*) \\leq g(c), \\quad \\forall c \\in \\mathbb{R}^{n+1}.$$\n",
    "\n",
    "From multivariable calculus, we know that any **local minimizer** $c^*$ much satisfy $\\nabla g(c^*) = 0$.\n",
    "\n",
    "It is not too difficult to show that \n",
    "\n",
    "$$\\nabla g(c) = 2Bc - 2b.$$\n",
    "\n",
    "Therefore, we just need to solve the following linear system:\n",
    "\n",
    "$$Bc^* = b.$$\n",
    "\n",
    "This linear system is known as the **normal equations** for the **continuous least-squares problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's find the polynomial of degree at most $n$ that best fits $f(x) = \\cos(x)$ over the interval $[0, 2\\pi]$.\n",
    "\n",
    "Using the **monomial basis** $\\set{1,x,x^2,\\ldots,x^n}$, we have \n",
    "\n",
    "$$B_{ij} = \\ip{x^i}{x^j} = \\int_0^{2\\pi} x^i x^j dx = \\frac{(2\\pi)^{i+j+1}}{i+j+1}, \\quad i,j = 0,\\ldots,n,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$b_i = \\ip{x^i}{\\cos(x)} = \\int_0^{2\\pi} x^i \\cos(x)\\,dx, \\quad i = 0,\\ldots,n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SymPy\n",
    "\n",
    "n = 2\n",
    "B = Sym[(2PI)^(i+j+1)/(i+j+1) for i=0:n, j=0:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `SymPy` to evaluate the vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")\n",
    "\n",
    "bsym = Sym[integrate(x^i*cos(x), 0, 2PI) for i=0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csym = map(simplify, B\\bsym)\n",
    "#csym = B\\bsym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the numerical integration function `quadgk` (will study these methods in Chapter 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuadGK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?quadgk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnum = Float64[quadgk(x -> x^i*cos(x), 0, 2Ï€)[1] for i=0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = float(bsym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b - bnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = float(B)\\b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(csym) - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function continuous_lsqr(f, a, b, n) \n",
    "    B = [quadgk(x -> x^i*x^j, a, b)[1] for i=0:n, j=0:n]\n",
    "    b = [quadgk(x -> x^i*f(x), a, b)[1] for i=0:n]\n",
    "    c = B\\b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = continuous_lsqr(cos, 0, 2Ï€, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LaTeXStrings\n",
    "\n",
    "n = 2\n",
    "\n",
    "xx = range(0, 2Ï€, length=1000)\n",
    "\n",
    "plot(legend=:bottomright, aspect_ratio=:equal, size=(800,600))\n",
    "plot!(xx, cos.(xx), label=L\"y = \\cos x\")\n",
    "\n",
    "# Evaluate p using Horner's rule\n",
    "c = continuous_lsqr(cos, 0, 2Ï€, n)\n",
    "p = zeros(length(xx))\n",
    "for i=n+1:-1:1\n",
    "    p = p.*xx .+ c[i]\n",
    "end\n",
    "\n",
    "plot!(xx, p, label=L\"y = p(x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique solution to the normal equations\n",
    "\n",
    "In addition, the matrix $B$ is **symmetric** and **positive definite**. \n",
    "\n",
    "It is clear that $B$ is symmetric since $\\ip{\\phi_j}{\\phi_k} = \\ip{\\phi_k}{\\phi_j}$.\n",
    "\n",
    "To show $B$ is positive definite, we let $c \\in \\mathbb{R}^{n+1}$ be nonzero, and note that \n",
    "\n",
    "$$c^T B c = \\ip{p}{p} = \\norm{p}_2^2,$$\n",
    "\n",
    "where $p = \\sum_{j=0}^n c_j \\phi_j$. Since $c \\neq 0$, we have that $p \\neq 0$ due to the fact the functions $\\phi_0,\\ldots,\\phi_n$ are **linearly independent**. Thus, $c^T B c = \\norm{p}_2^2 > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive definite matrices are nonsingular, so $B$ is nonsingular.\n",
    "\n",
    "The proof is easy. Suppose that $B$ is singular. Then there is a nonzero vector $c$ such that $Bc = 0$. Then we have\n",
    "\n",
    "$$c^T B c = 0,$$\n",
    "\n",
    "which contradicts the fact that $B$ is positive definite. Thus $B$ must be nonsingular.\n",
    "\n",
    "Thus, there is a **unique solution** $c^*$ to the normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique global minimizer\n",
    "\n",
    "Let $\\Delta c \\in \\mathbb{R}^{n+1}$ be nonzero. Then,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(c^* + \\Delta c) \n",
    "&=  (c^* + \\Delta c)^T B (c^* + \\Delta c) - 2b^T (c^* + \\Delta c) + \\ip{f}{f}\\\\\n",
    "&=  \\paren{c^*}^T B c^* + 2\\paren{\\Delta c}^T B c^* +  \\paren{\\Delta c}^T B \\Delta c - 2b^T c^* - 2b^T \\Delta c + \\ip{f}{f}\\\\\n",
    "&= g(c^*) + 2\\paren{\\Delta c}^T \\paren{B c^* - b} +  \\paren{\\Delta c}^T B \\Delta c \\\\\n",
    "&= g(c^*) +  \\paren{\\Delta c}^T B \\Delta c \\\\\n",
    "&> g(c^*). \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore, $g(c^*) < g(c^* + \\Delta c)$, for all $\\Delta c \\neq 0$, which implies that $c^*$ is the **unique global minimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projection\n",
    "\n",
    "Geometrically, $p = \\sum_{j=0}^n c_j^* \\phi_j$ is the **orthogonal projection** of $f$ onto $\\span \\set{\\phi_0,\\ldots,\\phi_n}$.\n",
    "\n",
    "We can see this by noting that the **residual function** $r = f - p$ is orthogonal to every basis vector $\\phi_i$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ip{r}{\\phi_i} \n",
    "&= \\ip{f-p}{\\phi_i} \\\\\n",
    "& = \\ip{f}{\\phi_i} - \\ip{p}{\\phi_i}\\\\\n",
    "& = b_i - \\ip{\\sum_{j=0}^n c_j^* \\phi_j}{\\phi_i}\\\\\n",
    "& = b_i - \\sum_{j=0}^n c_j^* \\ip{\\phi_j}{\\phi_i}\\\\\n",
    "& = b_i - \\sum_{j=0}^n c_j^* B_{ij}\\\\\n",
    "& = b_i - \\paren{B c^*}_i\\\\\n",
    "& = 0,\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "since $B c^* = b$. \n",
    "\n",
    "Indeed, the normal equations can be interpreted as $\\ip{f-p}{\\phi_i} = 0$, for $i=0,\\ldots,n$.\n",
    "\n",
    "Therefore, $r = f - p$ is orthogonal to the whole space $\\span\\set{\\phi_0,\\ldots,\\phi_n}$, and we conclude that $p$ is the orthogonal projection of $f$ onto $\\span\\set{\\phi_0,\\ldots,\\phi_n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons of using the monomial basis for continuous least squares\n",
    "\n",
    "### Pros:\n",
    "\n",
    "1. Simple. Over the interval $[a,b]$, we have:\n",
    "\n",
    "   $$B_{ij} = \\frac{(b-a)^{i+j+1}}{i+j+1}, \\quad i,j = 0,\\ldots,n.$$\n",
    "\n",
    "   Thus, $B$ is easy to evaluate. When $[a,b] = [0,1]$, we have the famous **[Hilbert matrix](http://en.wikipedia.org/wiki/Hilbert_matrix)**:\n",
    "\n",
    "  $$B = H_{n+1} =\n",
    "  \\begin{bmatrix}\n",
    "  1 & \\frac12 & \\cdots & \\frac{1}{n+1} \\\\\n",
    "  \\frac12 & \\frac13 & \\cdots & \\frac{1}{n+2} \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\frac{1}{n+1} & \\frac{1}{n+2} & \\cdots & \\frac{1}{2n + 1}\\\\\n",
    "  \\end{bmatrix}.\n",
    "  $$\n",
    "\n",
    "### Cons:\n",
    "\n",
    "1. Solving $B c = b$ can be expensive for large $n$.\n",
    "\n",
    "2. The matrix $B$ is highly **ill-conditioned** which means that the computed solution $c$ is highly sensitive to errors in the right-hand-side vector $b$ and to the round-off errors that occur during the computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
